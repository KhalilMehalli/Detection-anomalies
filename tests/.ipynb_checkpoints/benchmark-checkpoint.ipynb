{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fc678f-57a4-4c83-84aa-e5fff4ce5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import Bunch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8d043-a73a-4740-9576-e199adfed51b",
   "metadata": {},
   "source": [
    "### Datas preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670c3313-d670-4bf5-bd5f-7c2b9d5f547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data_rapport/speech.csv', '../data_rapport/kddcup99 http.csv', '../data_rapport/kddcup2014 donneurs .csv', '../data_rapport/satellite.csv', '../data_rapport/fraude.csv', '../data_rapport/cancer du sein.csv', '../data_rapport/shuttle.csv']\n"
     ]
    }
   ],
   "source": [
    "def csv_data_into_bunch(filenames):\n",
    "    \"\"\"\n",
    "    This function assume that the csv file will have label egal to 1 for anomalie and 0 for normal value.\n",
    "    The csv have a header too.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for i in filenames:\n",
    "        name = os.path.splitext(os.path.basename(i))[0]\n",
    "\n",
    "        data = pd.read_csv(i).to_numpy()\n",
    "    \n",
    "        x = data[:,:-1]\n",
    "        y = data[:,-1]\n",
    "\n",
    "        # Standardisation\n",
    "        scaler = StandardScaler()\n",
    "        x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "        # Store in a Bunch\n",
    "        datasets.append(Bunch(name=name, data=x_scaled, target=y))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "        \n",
    "def list_all_file_name_in_folder(folder=\"../data_rapport\"):\n",
    "    return [os.path.join(folder, filename) for filename in os.listdir(folder) if filename.endswith(\".csv\")]\n",
    "\n",
    "filenames = list_all_file_name_in_folder()\n",
    "print(filenames)\n",
    "\n",
    "datasets = csv_data_into_bunch(filenames)\n",
    "speech = datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bea4f0-fa66-4086-a99c-ae61f867213a",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "This function takes two parameters: `datasets` and `algorithms`.\n",
    "\n",
    "- `datasets`: a list of `Bunch` objects, each containing:\n",
    "  - `name`: the name of the dataset  \n",
    "  - `data`: the datas  \n",
    "  - `target`: the label \n",
    "\n",
    "\n",
    "\n",
    "- `algorithms`: a list of tuples in the following form:\n",
    "\n",
    "  ```python\n",
    "  algorithms = [\n",
    "      (\"name\", # Algorithm name \n",
    "       run_name, # runner function\n",
    "       {\"param1\": ..., \"param2\": ...}), # dict of paramaters \n",
    "      # …\n",
    "  ]\n",
    "\n",
    "This function will return three pandas dataFramme : auc, recall, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a6a6d4-1a09-4525-bfa2-0763759b18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(datasets, algorithms):\n",
    "    \"\"\"\n",
    "    Run a benchmark of multiple algorithms on multiple datasets.\n",
    "    \n",
    "     datasets : list of Bunch\n",
    "            Each Bunch must have attributes:\n",
    "              - name : dataset name\n",
    "              - data : feature data\n",
    "              - target : ground truth labels\n",
    "        algorithms : list of tuples\n",
    "            Each tuple is (algo_name, runner_function, params_dict):\n",
    "              - algo_name : descriptive name of the algorithm\n",
    "              - runner_function : function(bunch, params) - > dict\n",
    "              - params_dict: parameters to pass to the runner\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_auc : pandas.DataFrame\n",
    "        AUC scores for each (dataset × algorithm).\n",
    "    df_recall : pandas.DataFrame\n",
    "        Recall scores for each (dataset × algorithm).\n",
    "    df_time : pandas.DataFrame\n",
    "        Execution time (seconds) for each (dataset × algorithm).\n",
    "    \"\"\"\n",
    "    \n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for bunch in datasets:\n",
    "        print(bunch.name, \" : départ\")\n",
    "        auc_info, recall_info, time_info = {\"dataset\": bunch.name}, {\"dataset\": bunch.name}, {\"dataset\": bunch.name}\n",
    "        \n",
    "        for algo_name, algo_runner, params in algorithms:\n",
    "            print(\"  \", algo_name, \": départ\")\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[algo_name]    = res['auc']\n",
    "            recall_info[algo_name] = res['recall']\n",
    "            time_info[algo_name]   = res['time']\n",
    "            print(\"AUC :\", res['auc'], \"RAPPEL :\", res['recall'],\"TEMPS :\", res['time'])\n",
    "            print(\"  \", algo_name, \": fini\")\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "        print(bunch.name, \" : fini\")\n",
    "        \n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "    \n",
    "    return df_auc, df_recall, df_time\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986e4b9-bc87-4c78-8869-8f61fbfd9b35",
   "metadata": {},
   "source": [
    "### Algorithms \n",
    "\n",
    "Each runner function must accept exactly two arguments and return a dictionary with the same keys:\n",
    "\n",
    "\n",
    "- `Parameters`\n",
    "\n",
    "    - `bunch` : like in the datasets list of the benchmark parameters\n",
    "    \n",
    "    - `params` :  like in the algorithms list of the benchmark parameters\n",
    "\n",
    "- `Returns`\n",
    "\n",
    "- `dict`  \n",
    "  ```python\n",
    "  {\n",
    "    \"auc\":    <float>,  // ROC AUC score\n",
    "    \"recall\": <float>,  // Recall score\n",
    "    \"time\":   <float>   // Elapsed time (seconds)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0a6219-ac47-4f29-9b5f-4979470dcf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iforest(bunch, params):\n",
    "    \"\"\"\n",
    "    Fit and evaluate IsolationForest on a single dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bunch : Bunch\n",
    "        Must have attributes:\n",
    "          - data: feature data\n",
    "          - target: ground truth labels\n",
    "    params : dict\n",
    "        Keyword arguments for IsolationForest constructor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "          'auc': float,      # ROC AUC score\n",
    "          'recall': float,   # recall score\n",
    "          'time': float      # elapsed time in seconds\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    IF = IsolationForest(**params)\n",
    "    print(type(IF))\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    IF.fit(x)\n",
    "    y_pred = IF.predict(x) \n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -IF.decision_function(x)\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }\n",
    "\n",
    "\n",
    "def run_lof(bunch, params) :\n",
    "    \"\"\"\n",
    "    Fit and evaluate LocalOutlierFactor on a single dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bunch : Bunch\n",
    "        Must have attributes:\n",
    "          - data: feature data\n",
    "          - target: ground truth labels\n",
    "    params : dict\n",
    "        Keyword arguments for LocalOutlierFactor constructor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "          'auc': float,      # ROC AUC score\n",
    "          'recall': float,   # recall score\n",
    "          'time': float      # elapsed time in seconds\n",
    "        }\n",
    "    \"\"\"\n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    LOF = LocalOutlierFactor(** params)\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    y_pred = LOF.fit_predict(x)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -LOF.negative_outlier_factor_\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0bdeba1-32ad-4eee-bc57-319e704972d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech  : départ\n",
      "   IForest : départ\n",
      "<class 'sklearn.ensemble._iforest.IsolationForest'>\n",
      "AUC : 0.4608253250423968 RAPPEL : 0.13114754098360656 TEMPS : 0.2788610110001173\n",
      "   IForest : fini\n",
      "   LOF : départ\n",
      "AUC : 0.47594347088750705 RAPPEL : 0.11475409836065574 TEMPS : 0.3211567149992334\n",
      "   LOF : fini\n",
      "speech  : fini\n"
     ]
    }
   ],
   "source": [
    "algorithms = [\n",
    "     (\"IForest\", run_iforest, {'n_estimators':200, 'contamination':0.1}),\n",
    "     (\"LOF\", run_lof, {'n_neighbors':250, 'contamination':0.1}),\n",
    "]\n",
    "\n",
    "\n",
    "#df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "df_auc, df_recall, df_time = benchmark([speech], algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a528a2a9-a67f-4948-8cbd-55e9733c2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hlines(latex):\n",
    "    return latex.replace(\"\\\\\\\\\", \"\\\\\\\\ \\\\hline\")\n",
    "        \n",
    "def pandas_to_latex(df_auc, df_recall, df_time):\n",
    "    \"\"\"\n",
    "    Convert three pandas DataFrames into LaTeX-formatted tables with horizontal lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_auc : pandas.DataFrame\n",
    "        DataFrame of AUC scores.\n",
    "    df_recall : pandas.DataFrame\n",
    "        DataFrame of recall scores.\n",
    "    df_time : pandas.DataFrame\n",
    "        DataFrame of execution times.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of str\n",
    "        A tuple (latex_auc, latex_recall, latex_time), where each element is the\n",
    "        LaTeX code for the corresponding DataFrame, with added \\\\hline commands.\n",
    "    \"\"\"\n",
    "    latex_auc = add_hlines(df_auc.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_recall = add_hlines(df_recall.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_time = add_hlines(df_time.to_latex(index=False,float_format=\"%.2f\"))\n",
    "\n",
    "    return latex_auc, latex_recall, latex_time\n",
    "\n",
    "latex_auc, latex_recall, latex_time = pandas_to_latex(df_auc, df_recall, df_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edfe7053-7fee-4b76-bc83-b264fa370534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_latex_in_file(latex_auc, latex_recall,latex_time, algorithms, filepath = \"./latex_results.txt\"):\n",
    "    \"\"\"\n",
    "    Prepend LaTeX tables and algorithm info to a text file with a timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latex_auc : str\n",
    "        LaTeX code for the AUC table.\n",
    "    latex_recall : str\n",
    "        LaTeX code for the recall table.\n",
    "    latex_time : str\n",
    "        LaTeX code for the time table.\n",
    "    algorithms : list of tuples\n",
    "        Same list passed to benchmark, each tuple is\n",
    "        (name, runner_function, params_dict).\n",
    "    filepath : str, optional\n",
    "        Path to the output text file (default \"./latex_results.txt\").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The current timestamp is written at the top.\n",
    "    - Algorithm names and their parameter settings are listed.\n",
    "    - New content is prepended to preserve previous runs.\n",
    "    \"\"\"\n",
    "    # Actual time\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Informations about algorithms used : name1 : param1 = 0, param2=0 ; ...\n",
    "    infos = []\n",
    "    for name, _, params in algorithms:\n",
    "        param_str = \", \".join(f\"{k}={v}\" for k, v in params.items())\n",
    "        infos.append(f\"{name}: {param_str}\")\n",
    "    info_algo = \"; \".join(infos)\n",
    "\n",
    "    content = (\n",
    "        f\"{now}\\n\"\n",
    "        f\"{info_algo}\\n\\n\"\n",
    "        r\"%=== AUC table ===\" \"\\n\"\n",
    "        f\"{latex_auc}\\n\\n\"\n",
    "        r\"%=== Recall table ===\" \"\\n\"\n",
    "        f\"{latex_recall}\\n\\n\"\n",
    "        r\"%=== Time table ===\" \"\\n\"\n",
    "        f\"{latex_time}\\n\"\n",
    "    )\n",
    "\n",
    "    # Add at the start of the file without delete the old content \n",
    "    old = \"\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            old = f.read()\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "            f.write(old)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae97943-c58e-49e8-81b4-57b13281e5a9",
   "metadata": {},
   "source": [
    "### Combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9bece84-38c4-4ae3-b28f-26f3ed4b142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatisatison(datasets, algorithms, filepath = \"./latex_results.txt\"):\n",
    "    \"\"\"\n",
    "    Run the full pipeline: benchmark algorithms, convert results to LaTeX, and write to file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list of Bunch\n",
    "        List of dataset objects (each with .name, .data, .target).\n",
    "    algorithms : list of tuples\n",
    "        Each tuple is (name, runner_function, params_dict), as for benchmark().\n",
    "    filepath : str, optional\n",
    "        Path where the LaTeX output will be written (default \"./latex_results.txt\").\n",
    "\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Calculate all the metrics\n",
    "    df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "\n",
    "    # Transform the metrics into latex tab \n",
    "    latex_auc, latex_recall, latex_time = pandas_to_latex(df_auc, df_recall, df_time)\n",
    "\n",
    "    # Write these latex tab in a file\n",
    "    write_latex_in_file(latex_auc, latex_recall,latex_time, algorithms, filepath)\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b526246-4be2-4912-9383-24ec56cd55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "automatisation(datasets, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca8459-e751-4bff-9260-d421cb7d5d56",
   "metadata": {},
   "source": [
    "### PRINT, CSV, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e622d72-16bc-4c96-b8f4-3b3c4bfef6d8",
   "metadata": {},
   "source": [
    "python3 benchmark.py 2>&1 | tee prints_and_errors.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cee35784-525d-4a90-9a65-c8685e97e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_to_csv(df_auc, df_recall, df_time, name_csv, new_folder_path=\"./csv_results/\"):\n",
    "    # Creation of a new folder\n",
    "    os.makedirs(new_folder_path + name_csv, exist_ok=True)\n",
    "    \n",
    "    df_auc.to_csv(new_folder_path + name_csv + \"/\" +name_csv + \"_auc.csv\", index=False, header=True, float_format='%.2f')\n",
    "    df_recall.to_csv(new_folder_path + name_csv + \"/\" +name_csv +\"_recall.csv\", index=False, header=True,  float_format='%.2f')\n",
    "    df_time.to_csv(new_folder_path + name_csv + \"/\" +name_csv +\"_time.csv\", index=False, header=True, float_format='%.2f')\n",
    "\n",
    "pandas_to_csv(df_auc, df_recall, df_time, \"IF_LOF_Cancer_Speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4e228-c426-481a-b0c2-8fbf6b9013eb",
   "metadata": {},
   "source": [
    "### NaN Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e27bee-96ee-49fb-bba1-fa92caff7402",
   "metadata": {},
   "source": [
    "```python \n",
    "Traceback (most recent call last):\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 483, in <module>\n",
    "    automatisation(datasets, algorithms)\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 432, in automatisation\n",
    "    df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 308, in benchmark\n",
    "    res = algo_runner(bunch, params)\n",
    "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 184, in run_vae\n",
    "    VAE_model.fit(x)\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/base_dl.py\", line 202, in fit\n",
    "    self.decision_scores_ = self.decision_function(X)\n",
    "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/base_dl.py\", line 290, in decision_function\n",
    "    anomaly_scores = self.evaluate(data_loader)\n",
    "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/base_dl.py\", line 315, in evaluate\n",
    "    score = self.evaluating_forward(batch_data)\n",
    "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/vae.py\", line 264, in evaluating_forward\n",
    "    score = pairwise_distances_no_broadcast(x.numpy(),\n",
    "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/utils/stat_models.py\", line 43, in pairwise_distances_no_broadcast\n",
    "    Y = check_array(Y)\n",
    "        ^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
    "    _assert_all_finite(\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
    "    _assert_all_finite_element_wise(\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
    "    raise ValueError(msg_err)\n",
    "ValueError: Input contains NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb63ac0-4a21-428d-8a28-52551c477f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech has NaN?  False\n",
      "satellite has NaN?  False\n",
      "fraude has NaN?  False\n",
      "cancer du sein has NaN?  False\n",
      "shuttle has NaN?  False\n"
     ]
    }
   ],
   "source": [
    "for bunch in datasets:\n",
    "    has_nan = np.isnan(bunch.data).any()\n",
    "    print(f\"{bunch.name} has NaN?  {has_nan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4a397-64ef-4f01-ac05-043a98c1f24f",
   "metadata": {},
   "source": [
    "### Function Benchmark 2, where we do for each algo all the datasets first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ea813-3490-44ce-b592-aa39143dd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(datasets, algorithms):\n",
    "    \"\"\"\n",
    "    Run a benchmark of multiple algorithms on multiple datasets.\n",
    "    \n",
    "     datasets : list of Bunch\n",
    "            Each Bunch must have attributes:\n",
    "              - name : dataset name\n",
    "              - data : feature data\n",
    "              - target : ground truth labels\n",
    "        algorithms : list of tuples\n",
    "            Each tuple is (algo_name, runner_function, params_dict):\n",
    "              - algo_name : descriptive name of the algorithm\n",
    "              - runner_function : function(bunch, params) - > dict\n",
    "              - params_dict: parameters to pass to the runner\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_auc : pandas.DataFrame\n",
    "        AUC scores for each (dataset × algorithm).\n",
    "    df_recall : pandas.DataFrame\n",
    "        Recall scores for each (dataset × algorithm).\n",
    "    df_time : pandas.DataFrame\n",
    "        Execution time (seconds) for each (dataset × algorithm).\n",
    "    \"\"\"\n",
    "    \n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for algo_name, algo_runner, params in algorithms:\n",
    "        print(algo_name, \" : départ\")\n",
    "        auc_info, recall_info, time_info = {\"dataset\": bunch.name}, {\"dataset\": bunch.name}, {\"dataset\": bunch.name}\n",
    "        for bunch in datasets:\n",
    "        \n",
    "            print(\"  \", algo_name, \": départ\")\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[algo_name]    = res['auc']\n",
    "            recall_info[algo_name] = res['recall']\n",
    "            time_info[algo_name]   = res['time']\n",
    "            print(\"AUC :\", res['auc'], \"RAPPEL :\", res['recall'],\"TEMPS :\", res['time'])\n",
    "            print(\"  \", algo_name, \": fini\")\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "        print(bunch.name, \" : fini\")\n",
    "        \n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "    \n",
    "    return df_auc, df_recall, df_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
