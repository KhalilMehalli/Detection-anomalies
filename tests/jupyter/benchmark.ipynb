{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fc678f-57a4-4c83-84aa-e5fff4ce5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import Bunch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8d043-a73a-4740-9576-e199adfed51b",
   "metadata": {},
   "source": [
    "### Datas preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670c3313-d670-4bf5-bd5f-7c2b9d5f547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data_rapport/speech.csv', '../../data_rapport/cancerS.csv', '../../data_rapport/donneurs.csv', '../../data_rapport/satellite.csv', '../../data_rapport/fraude.csv', '../../data_rapport/http.csv', '../../data_rapport/shuttle.csv']\n"
     ]
    }
   ],
   "source": [
    "def csv_data_into_bunch(filenames):\n",
    "    \"\"\"\n",
    "    This function assume that the csv file will have label egal to 1 for anomalie and 0 for normal value.\n",
    "    The csv have a header too.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for i in filenames:\n",
    "        name = os.path.splitext(os.path.basename(i))[0]\n",
    "\n",
    "        data = pd.read_csv(i).to_numpy()\n",
    "    \n",
    "        x = data[:,:-1]\n",
    "        y = data[:,-1]\n",
    "\n",
    "        # Standardisation\n",
    "        scaler = StandardScaler()\n",
    "        x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "        # Store in a Bunch\n",
    "        datasets.append(Bunch(name=name, data=x_scaled, target=y))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "        \n",
    "def list_all_file_name_in_folder(folder=\"../../data_rapport\"):\n",
    "    return [os.path.join(folder, filename) for filename in os.listdir(folder) if filename.endswith(\".csv\")]\n",
    "\n",
    "filenames = list_all_file_name_in_folder()\n",
    "print(filenames)\n",
    "\n",
    "datasets = csv_data_into_bunch(filenames)\n",
    "speech = datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bea4f0-fa66-4086-a99c-ae61f867213a",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "This function takes two parameters: `datasets` and `algorithms`.\n",
    "\n",
    "- `datasets`: a list of `Bunch` objects, each containing:\n",
    "  - `name`: the name of the dataset  \n",
    "  - `data`: the datas  \n",
    "  - `target`: the label \n",
    "\n",
    "\n",
    "\n",
    "- `algorithms`: a list of tuples in the following form:\n",
    "\n",
    "  ```python\n",
    "  algorithms = [\n",
    "      (\"name\", # Algorithm name \n",
    "       run_name, # runner function\n",
    "       {\"param1\": ..., \"param2\": ...}), # dict of paramaters \n",
    "      # …\n",
    "  ]\n",
    "\n",
    "This function will return three pandas dataFramme : auc, recall, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a6a6d4-1a09-4525-bfa2-0763759b18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(datasets, algorithms):\n",
    "    \"\"\"\n",
    "    Run a benchmark of multiple algorithms on multiple datasets.\n",
    "    \n",
    "     datasets : list of Bunch\n",
    "            Each Bunch must have attributes:\n",
    "              - name : dataset name\n",
    "              - data : feature data\n",
    "              - target : ground truth labels\n",
    "        algorithms : list of tuples\n",
    "            Each tuple is (algo_name, runner_function, params_dict):\n",
    "              - algo_name : descriptive name of the algorithm\n",
    "              - runner_function : function(bunch, params) - > dict\n",
    "              - params_dict: parameters to pass to the runner\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_auc : pandas.DataFrame\n",
    "        AUC scores for each (dataset × algorithm).\n",
    "    df_recall : pandas.DataFrame\n",
    "        Recall scores for each (dataset × algorithm).\n",
    "    df_time : pandas.DataFrame\n",
    "        Execution time (seconds) for each (dataset × algorithm).\n",
    "    \"\"\"\n",
    "    \n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for bunch in datasets:\n",
    "        print(bunch.name, \" : départ\")\n",
    "        auc_info, recall_info, time_info = {\"dataset\": bunch.name}, {\"dataset\": bunch.name}, {\"dataset\": bunch.name}\n",
    "        \n",
    "        for algo_name, algo_runner, params in algorithms:\n",
    "            print(\"  \", algo_name, \": départ\")\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[algo_name]    = res['auc']\n",
    "            recall_info[algo_name] = res['recall']\n",
    "            time_info[algo_name]   = res['time']\n",
    "            \n",
    "            print(\"AUC :\", res['auc'], \"RAPPEL :\", res['recall'],\"TEMPS :\", res['time'])\n",
    "            print(\"  \", algo_name, \": fini\")\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "        \n",
    "        print(bunch.name, \" : fini\")\n",
    "        \n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "    \n",
    "    return df_auc, df_recall, df_time\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986e4b9-bc87-4c78-8869-8f61fbfd9b35",
   "metadata": {},
   "source": [
    "### Algorithms \n",
    "\n",
    "Each runner function must accept exactly two arguments and return a dictionary with the same keys:\n",
    "\n",
    "\n",
    "- `Parameters`\n",
    "\n",
    "    - `bunch` : like in the datasets list of the benchmark parameters\n",
    "    \n",
    "    - `params` :  like in the algorithms list of the benchmark parameters\n",
    "\n",
    "- `Returns`\n",
    "\n",
    "- `dict`  \n",
    "  ```python\n",
    "  {\n",
    "    \"auc\":    <float>,  // ROC AUC score\n",
    "    \"recall\": <float>,  // Recall score\n",
    "    \"time\":   <float>   // Elapsed time (seconds)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0a6219-ac47-4f29-9b5f-4979470dcf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iforest(bunch, params):\n",
    "    \"\"\"\n",
    "    Fit and evaluate IsolationForest on a single dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bunch : Bunch\n",
    "        Must have attributes:\n",
    "          - data: feature data\n",
    "          - target: ground truth labels\n",
    "    params : dict\n",
    "        Keyword arguments for IsolationForest constructor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "          'auc': float,      # ROC AUC score\n",
    "          'recall': float,   # recall score\n",
    "          'time': float      # elapsed time in seconds\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    IF = IsolationForest(**params)\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    IF.fit(x)\n",
    "    y_pred = IF.predict(x) \n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -IF.decision_function(x)\n",
    "\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }\n",
    "\n",
    "\n",
    "def run_lof(bunch, params) :\n",
    "    \"\"\"\n",
    "    Fit and evaluate LocalOutlierFactor on a single dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bunch : Bunch\n",
    "        Must have attributes:\n",
    "          - data: feature data\n",
    "          - target: ground truth labels\n",
    "    params : dict\n",
    "        Keyword arguments for LocalOutlierFactor constructor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "          'auc': float,      # ROC AUC score\n",
    "          'recall': float,   # recall score\n",
    "          'time': float      # elapsed time in seconds\n",
    "        }\n",
    "    \"\"\"\n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    LOF = LocalOutlierFactor(** params)\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    y_pred = LOF.fit_predict(x)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -LOF.negative_outlier_factor_\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bdeba1-32ad-4eee-bc57-319e704972d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "     (\"IForest\", run_iforest, {}),\n",
    "     (\"LOF\", run_lof, {}),\n",
    "]\n",
    "\n",
    "\n",
    "#df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "#df_auc, df_recall, df_time = benchmark([speech], algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a528a2a9-a67f-4948-8cbd-55e9733c2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hlines(latex):\n",
    "    return latex.replace(\"\\\\\\\\\", \"\\\\\\\\ \\\\hline\")\n",
    "        \n",
    "def pandas_to_latex(df_auc, df_recall, df_time):\n",
    "    \"\"\"\n",
    "    Convert three pandas DataFrames into LaTeX-formatted tables with horizontal lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_auc : pandas.DataFrame\n",
    "        DataFrame of AUC scores.\n",
    "    df_recall : pandas.DataFrame\n",
    "        DataFrame of recall scores.\n",
    "    df_time : pandas.DataFrame\n",
    "        DataFrame of execution times.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of str\n",
    "        A tuple (latex_auc, latex_recall, latex_time), where each element is the\n",
    "        LaTeX code for the corresponding DataFrame, with added \\\\hline commands.\n",
    "    \"\"\"\n",
    "    latex_auc = add_hlines(df_auc.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_recall = add_hlines(df_recall.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_time = add_hlines(df_time.to_latex(index=False,float_format=\"%.2f\"))\n",
    "\n",
    "    return latex_auc, latex_recall, latex_time\n",
    "\n",
    "#latex_auc, latex_recall, latex_time = pandas_to_latex(df_auc, df_recall, df_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfe7053-7fee-4b76-bc83-b264fa370534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_latex_in_file(latex_auc, latex_recall,latex_time, algorithms, filepath = \"./latex_results.txt\"):\n",
    "    \"\"\"\n",
    "    Prepend LaTeX tables and algorithm info to a text file with a timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latex_auc : str\n",
    "        LaTeX code for the AUC table.\n",
    "    latex_recall : str\n",
    "        LaTeX code for the recall table.\n",
    "    latex_time : str\n",
    "        LaTeX code for the time table.\n",
    "    algorithms : list of tuples\n",
    "        Same list passed to benchmark, each tuple is\n",
    "        (name, runner_function, params_dict).\n",
    "    filepath : str, optional\n",
    "        Path to the output text file (default \"./latex_results.txt\").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The current timestamp is written at the top.\n",
    "    - Algorithm names and their parameter settings are listed.\n",
    "    - New content is prepended to preserve previous runs.\n",
    "    \"\"\"\n",
    "    # Actual time\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Informations about algorithms used : name1 : param1 = 0, param2=0 ; ...\n",
    "    infos = []\n",
    "    for name, _, params in algorithms:\n",
    "        param_str = \", \".join(f\"{k}={v}\" for k, v in params.items())\n",
    "        infos.append(f\"{name}: {param_str}\")\n",
    "    info_algo = \"; \".join(infos)\n",
    "\n",
    "    content = (\n",
    "        f\"{now}\\n\"\n",
    "        f\"{info_algo}\\n\\n\"\n",
    "        r\"%=== AUC table ===\" \"\\n\"\n",
    "        f\"{latex_auc}\\n\\n\"\n",
    "        r\"%=== Recall table ===\" \"\\n\"\n",
    "        f\"{latex_recall}\\n\\n\"\n",
    "        r\"%=== Time table ===\" \"\\n\"\n",
    "        f\"{latex_time}\\n\"\n",
    "    )\n",
    "\n",
    "    # Add at the start of the file without delete the old content \n",
    "    old = \"\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            old = f.read()\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "            f.write(old)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae97943-c58e-49e8-81b4-57b13281e5a9",
   "metadata": {},
   "source": [
    "### Combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9bece84-38c4-4ae3-b28f-26f3ed4b142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatisatison(datasets, algorithms, filepath = \"./latex_results.txt\"):\n",
    "    \"\"\"\n",
    "    Run the full pipeline: benchmark algorithms, convert results to LaTeX, and write to file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list of Bunch\n",
    "        List of dataset objects (each with .name, .data, .target).\n",
    "    algorithms : list of tuples\n",
    "        Each tuple is (name, runner_function, params_dict), as for benchmark().\n",
    "    filepath : str, optional\n",
    "        Path where the LaTeX output will be written (default \"./latex_results.txt\").\n",
    "\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Calculate all the metrics\n",
    "    df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "\n",
    "    # Transform the metrics into latex tab \n",
    "    latex_auc, latex_recall, latex_time = pandas_to_latex(df_auc, df_recall, df_time)\n",
    "\n",
    "    # Write these latex tab in a file\n",
    "    write_latex_in_file(latex_auc, latex_recall,latex_time, algorithms, filepath)\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b526246-4be2-4912-9383-24ec56cd55f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'automatisation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mautomatisation\u001b[49m(datasets, algorithms)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'automatisation' is not defined"
     ]
    }
   ],
   "source": [
    "automatisatison(datasets, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca8459-e751-4bff-9260-d421cb7d5d56",
   "metadata": {},
   "source": [
    "### PRINT, CSV, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e622d72-16bc-4c96-b8f4-3b3c4bfef6d8",
   "metadata": {},
   "source": [
    "python3 benchmark.py 2>&1 | tee -a prints_and_errors.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cee35784-525d-4a90-9a65-c8685e97e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_to_csv(df_auc, df_recall, df_time, name_csv, new_folder_path=\"./csv_results/\"):\n",
    "    # Creation of a new folder\n",
    "    os.makedirs(new_folder_path + name_csv, exist_ok=True)\n",
    "    \n",
    "    df_auc.to_csv(new_folder_path + name_csv + \"/\" +name_csv + \"_auc.csv\", index=False, header=True, float_format='%.2f')\n",
    "    df_recall.to_csv(new_folder_path + name_csv + \"/\" +name_csv +\"_recall.csv\", index=False, header=True,  float_format='%.2f')\n",
    "    df_time.to_csv(new_folder_path + name_csv + \"/\" +name_csv +\"_time.csv\", index=False, header=True, float_format='%.2f')\n",
    "\n",
    "pandas_to_csv(df_auc, df_recall, df_time, \"IF_LOF_Cancer_Speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4e228-c426-481a-b0c2-8fbf6b9013eb",
   "metadata": {},
   "source": [
    "### NaN Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e27bee-96ee-49fb-bba1-fa92caff7402",
   "metadata": {},
   "source": [
    "```python \n",
    "Traceback (most recent call last):\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 483, in <module>\n",
    "    automatisation(datasets, algorithms)\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 432, in automatisation\n",
    "    df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 308, in benchmark\n",
    "    res = algo_runner(bunch, params)\n",
    "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 184, in run_vae\n",
    "    VAE_model.fit(x)\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/base_dl.py\", line 202, in fit\n",
    "    self.decision_scores_ = self.decision_function(X)\n",
    "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/base_dl.py\", line 290, in decision_function\n",
    "    anomaly_scores = self.evaluate(data_loader)\n",
    "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/base_dl.py\", line 315, in evaluate\n",
    "    score = self.evaluating_forward(batch_data)\n",
    "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/vae.py\", line 264, in evaluating_forward\n",
    "    score = pairwise_distances_no_broadcast(x.numpy(),\n",
    "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/utils/stat_models.py\", line 43, in pairwise_distances_no_broadcast\n",
    "    Y = check_array(Y)\n",
    "        ^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
    "    _assert_all_finite(\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
    "    _assert_all_finite_element_wise(\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
    "    raise ValueError(msg_err)\n",
    "ValueError: Input contains NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb63ac0-4a21-428d-8a28-52551c477f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech has NaN?  False\n",
      "satellite has NaN?  False\n",
      "fraude has NaN?  False\n",
      "cancer du sein has NaN?  False\n",
      "shuttle has NaN?  False\n"
     ]
    }
   ],
   "source": [
    "for bunch in datasets:\n",
    "    has_nan = np.isnan(bunch.data).any()\n",
    "    print(f\"{bunch.name} has NaN?  {has_nan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f431c-a794-48bc-95f0-45f0e9d59826",
   "metadata": {},
   "source": [
    "### Benchmark with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0e9ceb0-c737-48ab-9a99-afe6e497d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint_csv(algorithms, auc_csv, recall_csv,time_csv):\n",
    "    os.makedirs(\"./checkpoints\", exist_ok=True) # Create the repertories \n",
    "    algo_names  = [name for name, _, _ in algorithms]\n",
    "    cols = [\"dataset\"] + algo_names\n",
    "    \n",
    "    # Create empty CSVs with header row\n",
    "    pd.DataFrame(columns=cols).to_csv(auc_csv,  mode=\"w\",  index=False)\n",
    "    pd.DataFrame(columns=cols).to_csv(recall_csv, mode=\"w\", index=False)\n",
    "    pd.DataFrame(columns=cols).to_csv(time_csv, mode=\"w\",  index=False)\n",
    "\n",
    "def append_dict_to_csv(row, path): \n",
    "    \"\"\"\n",
    "        Add a row in a csv file, I use pandas because it's easy to write and I have a small number of dataset\n",
    "    \"\"\"\n",
    "    pd.DataFrame([row]).to_csv(path, mode=\"a\", header=False, index=False, float_format=\"%.2f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a122c023-f8d9-4815-94d3-9f0c05a28e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_checkpoint_csv(algorithms,\"./checkpoints/auc_checkpoint.csv\", \"./checkpoints/recall_checkpoint.csv\",\"./checkpoints/time_checkpoint.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc63889-5021-4bdb-8a99-2db1d5f700ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech  : départ\n",
      "   IForest : départ\n",
      "AUC : 0.47671226681741097 RAPPEL : 0.01639344262295082 TEMPS : 0.1239963070001977\n",
      "   IForest : fini\n",
      "   LOF : départ\n",
      "AUC : 0.5078168456755229 RAPPEL : 0.0 TEMPS : 0.1987326070011477\n",
      "   LOF : fini\n",
      "speech  : fini\n",
      "cancerS  : départ\n",
      "   IForest : départ\n",
      "AUC : 0.973109243697479 RAPPEL : 0.8 TEMPS : 0.09633737500189454\n",
      "   IForest : fini\n",
      "   LOF : départ\n",
      "AUC : 0.9845938375350141 RAPPEL : 0.9 TEMPS : 0.0030664250007248484\n",
      "   LOF : fini\n",
      "cancerS  : fini\n",
      "donneurs  : départ\n",
      "   IForest : départ\n",
      "AUC : 0.7805672089358378 RAPPEL : 0.7719422500681014 TEMPS : 2.8159428920007485\n",
      "   IForest : fini\n",
      "   LOF : départ\n"
     ]
    }
   ],
   "source": [
    "def benchmark_checkpoints(datasets, algorithms, auc_csv=\"./checkpoints/auc_checkpoint.csv\", recall_csv=\"./checkpoints/recall_checkpoint.csv\",\n",
    "              time_csv=\"./checkpoints/time_checkpoint.csv\"):\n",
    "    \"\"\"\n",
    "    Run a benchmark of multiple algorithms on multiple datasets.\n",
    "    \n",
    "     datasets : list of Bunch\n",
    "            Each Bunch must have attributes:\n",
    "              - name : dataset name\n",
    "              - data : feature data\n",
    "              - target : ground truth labels\n",
    "        algorithms : list of tuples\n",
    "            Each tuple is (algo_name, runner_function, params_dict):\n",
    "              - algo_name : descriptive name of the algorithm\n",
    "              - runner_function : function(bunch, params) - > dict\n",
    "              - params_dict: parameters to pass to the runner\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_auc : pandas.DataFrame\n",
    "        AUC scores for each (dataset × algorithm).\n",
    "    df_recall : pandas.DataFrame\n",
    "        Recall scores for each (dataset × algorithm).\n",
    "    df_time : pandas.DataFrame\n",
    "        Execution time (seconds) for each (dataset × algorithm).\n",
    "    \"\"\"\n",
    "\n",
    "    create_checkpoint_csv(algorithms, auc_csv, recall_csv,time_csv) ### <- <-\n",
    "    \n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for bunch in datasets:\n",
    "        print(bunch.name, \" : départ\")\n",
    "        auc_info, recall_info, time_info = {\"dataset\": bunch.name}, {\"dataset\": bunch.name}, {\"dataset\": bunch.name}\n",
    "        \n",
    "        for algo_name, algo_runner, params in algorithms:\n",
    "            print(\"  \", algo_name, \": départ\")\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[algo_name]    = res['auc']\n",
    "            recall_info[algo_name] = res['recall']\n",
    "            time_info[algo_name]   = res['time']\n",
    "            \n",
    "            print(\"AUC :\", res['auc'], \"RAPPEL :\", res['recall'],\"TEMPS :\", res['time'])\n",
    "            print(\"  \", algo_name, \": fini\")\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "\n",
    "        append_dict_to_csv(auc_info,auc_csv) ### <- <-\n",
    "        append_dict_to_csv(recall_info,recall_csv)\n",
    "        append_dict_to_csv(time_info,time_csv)\n",
    "           \n",
    "        print(bunch.name, \" : fini\")\n",
    "\n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "    \n",
    "    return df_auc, df_recall, df_time\n",
    "\n",
    "\n",
    "benchmark_checkpoints(datasets, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4a397-64ef-4f01-ac05-043a98c1f24f",
   "metadata": {},
   "source": [
    "### Function Benchmark 2, where we do for each algo all the datasets first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d775f009-324e-42b1-b22b-5107b1577de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint_csv2(datasets, auc_csv, recall_csv,time_csv):\n",
    "    os.makedirs(\"./checkpoints\", exist_ok=True) # Create the repertories \n",
    "    dataset_names = [b.name for b in datasets]\n",
    "    cols = [\"algorithm\"] + dataset_names\n",
    "\n",
    "    # write empty CSVs with that header (mode=\"w\" to overwrite)\n",
    "    pd.DataFrame(columns=cols).to_csv(   auc_csv,    mode=\"w\", index=False)\n",
    "    pd.DataFrame(columns=cols).to_csv(   recall_csv, mode=\"w\", index=False)\n",
    "    pd.DataFrame(columns=cols).to_csv(   time_csv,   mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "351ea813-3490-44ce-b592-aa39143dd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_algo_dataset(datasets, algorithms, auc_csv=\"./checkpoints/auc_checkpoint.csv\", recall_csv=\"./checkpoints/recall_checkpoint.csv\",\n",
    "              time_csv=\"./checkpoints/time_checkpoint.csv\"):\n",
    "    \"\"\"\n",
    "    Run a benchmark of multiple algorithms on multiple datasets.\n",
    "    \n",
    "     datasets : list of Bunch\n",
    "            Each Bunch must have attributes:\n",
    "              - name : dataset name\n",
    "              - data : feature data\n",
    "              - target : ground truth labels\n",
    "        algorithms : list of tuples\n",
    "            Each tuple is (algo_name, runner_function, params_dict):\n",
    "              - algo_name : descriptive name of the algorithm\n",
    "              - runner_function : function(bunch, params) - > dict\n",
    "              - params_dict: parameters to pass to the runner\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_auc : pandas.DataFrame\n",
    "        AUC scores for each (dataset × algorithm).\n",
    "    df_recall : pandas.DataFrame\n",
    "        Recall scores for each (dataset × algorithm).\n",
    "    df_time : pandas.DataFrame\n",
    "        Execution time (seconds) for each (dataset × algorithm).\n",
    "    \"\"\"\n",
    "\n",
    "    create_checkpoint_csv2(datasets, auc_csv, recall_csv, time_csv)\n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for algo_name, algo_runner, params in algorithms:\n",
    "        print(algo_name, \" : départ\")\n",
    "        auc_info, recall_info, time_info    = {\"algorithm\": algo_name}, {\"algorithm\": algo_name},{\"algorithm\": algo_name}        \n",
    "        \n",
    "        for bunch in datasets:\n",
    "        \n",
    "            print(\"  \", bunch.name, \": départ\")\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[bunch.name]    = res[\"auc\"]\n",
    "            recall_info[bunch.name] = res[\"recall\"]\n",
    "            time_info[bunch.name]   = res[\"time\"]\n",
    "\n",
    "            print(\"  \", \"AUC :\", res['auc'], \"RAPPEL :\", res['recall'],\"TEMPS :\", res['time'])\n",
    "            print(\"  \", bunch.name, \": fini\")\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "        print(algo_name, \" : fini\")\n",
    "\n",
    "        append_dict_to_csv(auc_info,auc_csv) \n",
    "        append_dict_to_csv(recall_info,recall_csv)\n",
    "        append_dict_to_csv(time_info,time_csv)\n",
    "        \n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "    \n",
    "    return df_auc, df_recall, df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4335eec3-9c0b-4620-8afe-65d5e5909454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IForest  : départ\n",
      "   speech : départ\n",
      "   AUC : 0.4572707744488411 RAPPEL : 0.0 TEMPS : 0.15146792200539494\n",
      "   speech : fini\n",
      "   cancerS : départ\n",
      "   AUC : 0.9801120448179272 RAPPEL : 0.9 TEMPS : 0.10790896400430938\n",
      "   cancerS : fini\n",
      "   donneurs : départ\n",
      "   AUC : 0.7738386254193259 RAPPEL : 0.7811223099972759 TEMPS : 3.0673916560044745\n",
      "   donneurs : fini\n",
      "   satellite : départ\n",
      "   AUC : 0.945828192371476 RAPPEL : 0.7733333333333333 TEMPS : 0.112584368995158\n",
      "   satellite : fini\n",
      "   fraude : départ\n",
      "   AUC : 0.9473117208398047 RAPPEL : 0.8272357723577236 TEMPS : 1.204344095000124\n",
      "   fraude : fini\n",
      "   http : départ\n",
      "   AUC : 0.9679862992054658 RAPPEL : 0.8003802281368821 TEMPS : 2.090942137998354\n",
      "   http : fini\n",
      "   shuttle : départ\n",
      "   AUC : 0.9978325030253964 RAPPEL : 0.9806378132118451 TEMPS : 0.2797882910017506\n",
      "   shuttle : fini\n",
      "IForest  : fini\n",
      "IForest  : départ\n",
      "   speech : départ\n",
      "   AUC : 0.5023855285472019 RAPPEL : 0.0 TEMPS : 0.10912616900168359\n",
      "   speech : fini\n",
      "   cancerS : départ\n",
      "   AUC : 0.9890756302521009 RAPPEL : 1.0 TEMPS : 0.0864231299929088\n",
      "   cancerS : fini\n",
      "   donneurs : départ\n",
      "   AUC : 0.7987714785056659 RAPPEL : 0.8801144102424407 TEMPS : 2.8567574089975096\n",
      "   donneurs : fini\n",
      "   satellite : départ\n",
      "   AUC : 0.946276616915423 RAPPEL : 0.84 TEMPS : 0.1091974549999577\n",
      "   satellite : fini\n",
      "   fraude : départ\n",
      "   AUC : 0.9530236666390722 RAPPEL : 0.8353658536585366 TEMPS : 1.317130764000467\n",
      "   fraude : fini\n",
      "   http : départ\n",
      "   AUC : 0.9673553148117067 RAPPEL : 0.7994296577946768 TEMPS : 1.9126738190025208\n",
      "   http : fini\n",
      "   shuttle : départ\n",
      "   AUC : 0.9975557475934494 RAPPEL : 0.9851936218678815 TEMPS : 0.28711583200492896\n",
      "   shuttle : fini\n",
      "IForest  : fini\n"
     ]
    }
   ],
   "source": [
    "df1, df2, df3 =benchmark_algo_dataset(datasets, [algorithms[0],algorithms[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951965c6-28ca-4f46-a718-fd6645d9fbe7",
   "metadata": {},
   "source": [
    "### Rank directly in latex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0aaf003-6024-4664-8143-e29256a93978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hlines(latex):\n",
    "    return latex.replace(\"\\\\\\\\\", \"\\\\\\\\ \\\\hline\")\n",
    "    \n",
    "def pandas_to_latex_with_ranks(df, ascending):\n",
    "    \"\"\"\n",
    "    Convert a df to latex, adding per-row rankings and bolding the top value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas \n",
    "    ascending : bool\n",
    "        If True, rank 1 = smallest value; if False, rank 1 = largest value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "    \"\"\"\n",
    "    # Identify numeric columns to rank, and preserve all others\n",
    "    num_cols = df.select_dtypes(include=\"number\").columns\n",
    "    other_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "    # Round values to two decimals \n",
    "    vals = df[num_cols].round(2)\n",
    "\n",
    "    # Per row ranks, direction controlled by ascending\n",
    "    ranks = vals.rank(axis=1, method='dense', ascending=ascending).astype(int)\n",
    "\n",
    "    # Cconvert to strings, because the result will go in this dataset \n",
    "    vals = vals.astype(str)\n",
    "\n",
    "    # Combine each value with its rank, bolding n.1 \n",
    "    for idx in df.index:\n",
    "        for col in num_cols:\n",
    "            v = vals.at[idx, col]\n",
    "            r = ranks.at[idx, col]\n",
    "            \n",
    "            if r == 1: \n",
    "                cell = f\"\\\\textbf{{{v}}} ({r})\"\n",
    "            else:\n",
    "                cell = f\"{v} ({r})\"\n",
    "            vals.at[idx, col] = cell\n",
    "\n",
    "    df_final = pd.concat([df[other_cols], vals], axis=1)\n",
    "    \n",
    "    return add_hlines(df_final.to_latex(index=False, float_format=\"%.2f\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16400167-6e87-4ccc-97ef-4ed59e4680cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{llllllll}\\n\\\\toprule\\nalgorithm & speech & cancerS & donneurs & satellite & fraude & http & shuttle \\\\\\\\ \\\\hline\\n\\\\midrule\\nIForest & 0.0 (7) & 0.9 (2) & 0.78 (5) & 0.77 (6) & 0.83 (3) & 0.8 (4) & \\\\textbf{0.98} (1) \\\\\\\\ \\\\hline\\nIForest & 0.0 (6) & \\\\textbf{1.0} (1) & 0.88 (3) & 0.84 (4) & 0.84 (4) & 0.8 (5) & 0.99 (2) \\\\\\\\ \\\\hline\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_to_latex_with_ranks(df2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de7e68-9e00-495b-badb-0c1d8d9b481a",
   "metadata": {},
   "source": [
    "### FINAL, YAML file for the algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40bb4ab3-e7d6-4fc1-9ca7-38c46d128ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba692c0e-5d81-4627-af75-26f44ecd6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables_from_yaml(path=\"./infos.yaml\",algo_file=\"python.algorithms\"):\n",
    "    \"\"\"\n",
    "    Load a yaml file and return usable variables:\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : path of the yaml file\n",
    "    algo_file = path of the file which contains all the algo\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - datasets_folder : path to the datasets folder\n",
    "    - output_folder_checkpoints : path to the folder which store CSV checkpoints\n",
    "    - latex_output : path to the latex results file\n",
    "    - algorithms (list of tuples): (name, runner_function, params_dict)\n",
    "    \"\"\"\n",
    "    # Load YAML file \n",
    "    with open(path, \"r\") as f:\n",
    "        info = yaml.safe_load(f)\n",
    "\n",
    "    # Extract info \n",
    "    datasets_folder = info[\"datasets_folder\"]\n",
    "    output_folder_checkpoints = info.get(\"output_folder_checkpoints\", \"./checkpoints/\")\n",
    "    latex_output = info.get(\"latex_output\", \"./latex_results.txt\")\n",
    "\n",
    "    # Dynamically import the file where runner functions are defined\n",
    "    algo_module = importlib.import_module(algo_file)  # import the file python.algorithms.py\n",
    "\n",
    "    # Build the list of algorithm tuples\n",
    "    algorithms = []\n",
    "    for algo in info.get(\"algorithms\", []):\n",
    "        name = algo[\"name\"]\n",
    "        runner_fn = getattr(algo_module, algo[\"runner\"])\n",
    "        params = algo.get(\"params\", {})\n",
    "        algorithms.append((name, runner_fn, params))\n",
    "\n",
    "    return datasets_folder, output_folder_checkpoints, latex_output, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b02535c-7dbe-434b-b693-a936696dde08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mload_variables_from_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../infos.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36mload_variables_from_yaml\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     16\u001b[0m latex_output \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatex_output\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./latex_results.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Dynamically import the file where runner functions are defined\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m algo_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython.algorithms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# import the file python.algorithms.py\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Build the list of algorithm tuples\u001b[39;00m\n\u001b[1;32m     22\u001b[0m algorithms \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/ls/envs/sfml/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1310\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'python'"
     ]
    }
   ],
   "source": [
    "print(load_variables_from_yaml(\"../infos.yaml\"))\n",
    "    [\n",
    "        (\"IForest\", run_iforest, {\"random_state\":10}),\n",
    "        (\"LOF\",  run_lof, {}),\n",
    "        (\"KNN\", run_knn, {\"n_jobs\":-1}), # Jobs in parallel\n",
    "        (\"AE\", run_autoencoder, {\"random_state\":10, \"preprocessing\": False,\"epoch_num\": 10}),\n",
    "        (\"VAE\", run_vae, {\"random_state\":10, \"preprocessing\": False,\"epoch_num\": 10}),\n",
    "        (\"AE1SVM\", run_ae1svm, {\"preprocessing\": False,\"epochs\": 10}),\n",
    "        (\"OCSVM\", run_ocsvm, {}),\n",
    "        (\"DeepSVDD\", run_deepsvdd, {\"random_state\":10,\"preprocessing\": False,\"epochs\": 10}),\n",
    "        (\"AnoGan\", run_anogan, {\"preprocessing\": False,\"epochs\": 10})\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6fbc0-b0e0-43ca-bfe5-5d3ab1f83160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
