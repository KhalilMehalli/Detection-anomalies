{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fc678f-57a4-4c83-84aa-e5fff4ce5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import Bunch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8d043-a73a-4740-9576-e199adfed51b",
   "metadata": {},
   "source": [
    "### Datas preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670c3313-d670-4bf5-bd5f-7c2b9d5f547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data_rapport/speech.csv', '../../data_rapport/cancerS.csv', '../../data_rapport/donneurs.csv', '../../data_rapport/satellite.csv', '../../data_rapport/fraude.csv', '../../data_rapport/http.csv', '../../data_rapport/shuttle.csv']\n"
     ]
    }
   ],
   "source": [
    "def csv_data_into_bunch(filenames):\n",
    "    \"\"\"\n",
    "    This function assume that the csv file will have label egal to 1 for anomalie and 0 for normal value.\n",
    "    The csv have a header too.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for i in filenames:\n",
    "        name = os.path.splitext(os.path.basename(i))[0]\n",
    "\n",
    "        data = pd.read_csv(i).to_numpy()\n",
    "    \n",
    "        x = data[:,:-1]\n",
    "        y = data[:,-1]\n",
    "\n",
    "        # Standardisation\n",
    "        scaler = StandardScaler()\n",
    "        x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "        # Store in a Bunch\n",
    "        datasets.append(Bunch(name=name, data=x_scaled, target=y))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "        \n",
    "def list_all_file_name_in_folder(folder=\"../../data_rapport\"):\n",
    "    return [os.path.join(folder, filename) for filename in os.listdir(folder) if filename.endswith(\".csv\")]\n",
    "\n",
    "filenames = list_all_file_name_in_folder()\n",
    "print(filenames)\n",
    "\n",
    "datasets = csv_data_into_bunch(filenames)\n",
    "speech = datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bea4f0-fa66-4086-a99c-ae61f867213a",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "This function takes two parameters: `datasets` and `algorithms`.\n",
    "\n",
    "- `datasets`: a list of `Bunch` objects, each containing:\n",
    "  - `name`: the name of the dataset  \n",
    "  - `data`: the datas  \n",
    "  - `target`: the label \n",
    "\n",
    "\n",
    "\n",
    "- `algorithms`: a list of tuples in the following form:\n",
    "\n",
    "  ```python\n",
    "  algorithms = [\n",
    "      (\"name\", # Algorithm name \n",
    "       run_name, # runner function\n",
    "       {\"param1\": ..., \"param2\": ...}), # dict of paramaters \n",
    "      # …\n",
    "  ]\n",
    "\n",
    "This function will return three pandas dataFramme : auc, recall, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a6a6d4-1a09-4525-bfa2-0763759b18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(datasets, algorithms):\n",
    "    \"\"\"\n",
    "    Run a benchmark of multiple algorithms on multiple datasets.\n",
    "    \n",
    "     datasets : list of Bunch\n",
    "            Each Bunch must have attributes:\n",
    "              - name : dataset name\n",
    "              - data : feature data\n",
    "              - target : ground truth labels\n",
    "        algorithms : list of tuples\n",
    "            Each tuple is (algo_name, runner_function, params_dict):\n",
    "              - algo_name : descriptive name of the algorithm\n",
    "              - runner_function : function(bunch, params) - > dict\n",
    "              - params_dict: parameters to pass to the runner\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_auc : pandas.DataFrame\n",
    "        AUC scores for each (dataset × algorithm).\n",
    "    df_recall : pandas.DataFrame\n",
    "        Recall scores for each (dataset × algorithm).\n",
    "    df_time : pandas.DataFrame\n",
    "        Execution time (seconds) for each (dataset × algorithm).\n",
    "    \"\"\"\n",
    "    \n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for bunch in datasets:\n",
    "        print(bunch.name, \" : départ\")\n",
    "        auc_info, recall_info, time_info = {\"dataset\": bunch.name}, {\"dataset\": bunch.name}, {\"dataset\": bunch.name}\n",
    "        \n",
    "        for algo_name, algo_runner, params in algorithms:\n",
    "            print(\"  \", algo_name, \": départ\")\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[algo_name]    = res['auc']\n",
    "            recall_info[algo_name] = res['recall']\n",
    "            time_info[algo_name]   = res['time']\n",
    "            \n",
    "            print(\"AUC :\", res['auc'], \"RAPPEL :\", res['recall'],\"TEMPS :\", res['time'])\n",
    "            print(\"  \", algo_name, \": fini\")\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "        \n",
    "        print(bunch.name, \" : fini\")\n",
    "        \n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "    \n",
    "    return df_auc, df_recall, df_time\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986e4b9-bc87-4c78-8869-8f61fbfd9b35",
   "metadata": {},
   "source": [
    "### Algorithms \n",
    "\n",
    "Each runner function must accept exactly two arguments and return a dictionary with the same keys:\n",
    "\n",
    "\n",
    "- `Parameters`\n",
    "\n",
    "    - `bunch` : like in the datasets list of the benchmark parameters\n",
    "    \n",
    "    - `params` :  like in the algorithms list of the benchmark parameters\n",
    "\n",
    "- `Returns`\n",
    "\n",
    "- `dict`  \n",
    "  ```python\n",
    "  {\n",
    "    \"auc\":    <float>,  // ROC AUC score\n",
    "    \"recall\": <float>,  // Recall score\n",
    "    \"time\":   <float>   // Elapsed time (seconds)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0a6219-ac47-4f29-9b5f-4979470dcf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iforest(bunch, params):\n",
    "    \"\"\"\n",
    "    Fit and evaluate IsolationForest on a single dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bunch : Bunch\n",
    "        Must have attributes:\n",
    "          - data: feature data\n",
    "          - target: ground truth labels\n",
    "    params : dict\n",
    "        Keyword arguments for IsolationForest constructor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "          'auc': float,      # ROC AUC score\n",
    "          'recall': float,   # recall score\n",
    "          'time': float      # elapsed time in seconds\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    IF = IsolationForest(**params)\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    IF.fit(x)\n",
    "    y_pred = IF.predict(x) \n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -IF.decision_function(x)\n",
    "\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }\n",
    "\n",
    "\n",
    "def run_lof(bunch, params) :\n",
    "    \"\"\"\n",
    "    Fit and evaluate LocalOutlierFactor on a single dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bunch : Bunch\n",
    "        Must have attributes:\n",
    "          - data: feature data\n",
    "          - target: ground truth labels\n",
    "    params : dict\n",
    "        Keyword arguments for LocalOutlierFactor constructor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "          'auc': float,      # ROC AUC score\n",
    "          'recall': float,   # recall score\n",
    "          'time': float      # elapsed time in seconds\n",
    "        }\n",
    "    \"\"\"\n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    LOF = LocalOutlierFactor(** params)\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    y_pred = LOF.fit_predict(x)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -LOF.negative_outlier_factor_\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bdeba1-32ad-4eee-bc57-319e704972d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "     (\"IForest\", run_iforest, {}),\n",
    "     (\"LOF\", run_lof, {}),\n",
    "]\n",
    "\n",
    "\n",
    "#df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "#df_auc, df_recall, df_time = benchmark([speech], algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a528a2a9-a67f-4948-8cbd-55e9733c2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hlines(latex):\n",
    "    return latex.replace(\"\\\\\\\\\", \"\\\\\\\\ \\\\hline\")\n",
    "        \n",
    "def pandas_to_latex(df_auc, df_recall, df_time):\n",
    "    \"\"\"\n",
    "    Convert three pandas DataFrames into LaTeX-formatted tables with horizontal lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_auc : pandas.DataFrame\n",
    "        DataFrame of AUC scores.\n",
    "    df_recall : pandas.DataFrame\n",
    "        DataFrame of recall scores.\n",
    "    df_time : pandas.DataFrame\n",
    "        DataFrame of execution times.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of str\n",
    "        A tuple (latex_auc, latex_recall, latex_time), where each element is the\n",
    "        LaTeX code for the corresponding DataFrame, with added \\\\hline commands.\n",
    "    \"\"\"\n",
    "    latex_auc = add_hlines(df_auc.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_recall = add_hlines(df_recall.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_time = add_hlines(df_time.to_latex(index=False,float_format=\"%.2f\"))\n",
    "\n",
    "    return latex_auc, latex_recall, latex_time\n",
    "\n",
    "#latex_auc, latex_recall, latex_time = pandas_to_latex(df_auc, df_recall, df_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfe7053-7fee-4b76-bc83-b264fa370534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_latex_in_file(latex_auc, latex_recall,latex_time, algorithms, filepath = \"./latex_results.txt\"):\n",
    "    \"\"\"\n",
    "    Prepend LaTeX tables and algorithm info to a text file with a timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latex_auc : str\n",
    "        LaTeX code for the AUC table.\n",
    "    latex_recall : str\n",
    "        LaTeX code for the recall table.\n",
    "    latex_time : str\n",
    "        LaTeX code for the time table.\n",
    "    algorithms : list of tuples\n",
    "        Same list passed to benchmark, each tuple is\n",
    "        (name, runner_function, params_dict).\n",
    "    filepath : str, optional\n",
    "        Path to the output text file (default \"./latex_results.txt\").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The current timestamp is written at the top.\n",
    "    - Algorithm names and their parameter settings are listed.\n",
    "    - New content is prepended to preserve previous runs.\n",
    "    \"\"\"\n",
    "    # Actual time\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Informations about algorithms used : name1 : param1 = 0, param2=0 ; ...\n",
    "    infos = []\n",
    "    for name, _, params in algorithms:\n",
    "        param_str = \", \".join(f\"{k}={v}\" for k, v in params.items())\n",
    "        infos.append(f\"{name}: {param_str}\")\n",
    "    info_algo = \"; \".join(infos)\n",
    "\n",
    "    content = (\n",
    "        f\"{now}\\n\"\n",
    "        f\"{info_algo}\\n\\n\"\n",
    "        r\"%=== AUC table ===\" \"\\n\"\n",
    "        f\"{latex_auc}\\n\\n\"\n",
    "        r\"%=== Recall table ===\" \"\\n\"\n",
    "        f\"{latex_recall}\\n\\n\"\n",
    "        r\"%=== Time table ===\" \"\\n\"\n",
    "        f\"{latex_time}\\n\"\n",
    "    )\n",
    "\n",
    "    # Add at the start of the file without delete the old content \n",
    "    old = \"\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            old = f.read()\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "            f.write(old)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae97943-c58e-49e8-81b4-57b13281e5a9",
   "metadata": {},
   "source": [
    "### Combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9bece84-38c4-4ae3-b28f-26f3ed4b142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatisatison(datasets, algorithms, filepath = \"./latex_results.txt\"):\n",
    "    \"\"\"\n",
    "    Run the full pipeline: benchmark algorithms, convert results to LaTeX, and write to file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list of Bunch\n",
    "        List of dataset objects (each with .name, .data, .target).\n",
    "    algorithms : list of tuples\n",
    "        Each tuple is (name, runner_function, params_dict), as for benchmark().\n",
    "    filepath : str, optional\n",
    "        Path where the LaTeX output will be written (default \"./latex_results.txt\").\n",
    "\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Calculate all the metrics\n",
    "    df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "\n",
    "    # Transform the metrics into latex tab \n",
    "    latex_auc, latex_recall, latex_time = pandas_to_latex(df_auc, df_recall, df_time)\n",
    "\n",
    "    # Write these latex tab in a file\n",
    "    write_latex_in_file(latex_auc, latex_recall,latex_time, algorithms, filepath)\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b526246-4be2-4912-9383-24ec56cd55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "automatisation(datasets, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca8459-e751-4bff-9260-d421cb7d5d56",
   "metadata": {},
   "source": [
    "### PRINT, CSV, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e622d72-16bc-4c96-b8f4-3b3c4bfef6d8",
   "metadata": {},
   "source": [
    "python3 benchmark.py 2>&1 | tee prints_and_errors.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cee35784-525d-4a90-9a65-c8685e97e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_to_csv(df_auc, df_recall, df_time, name_csv, new_folder_path=\"./csv_results/\"):\n",
    "    # Creation of a new folder\n",
    "    os.makedirs(new_folder_path + name_csv, exist_ok=True)\n",
    "    \n",
    "    df_auc.to_csv(new_folder_path + name_csv + \"/\" +name_csv + \"_auc.csv\", index=False, header=True, float_format='%.2f')\n",
    "    df_recall.to_csv(new_folder_path + name_csv + \"/\" +name_csv +\"_recall.csv\", index=False, header=True,  float_format='%.2f')\n",
    "    df_time.to_csv(new_folder_path + name_csv + \"/\" +name_csv +\"_time.csv\", index=False, header=True, float_format='%.2f')\n",
    "\n",
    "pandas_to_csv(df_auc, df_recall, df_time, \"IF_LOF_Cancer_Speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4e228-c426-481a-b0c2-8fbf6b9013eb",
   "metadata": {},
   "source": [
    "### NaN Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e27bee-96ee-49fb-bba1-fa92caff7402",
   "metadata": {},
   "source": [
    "```python \n",
    "Traceback (most recent call last):\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 483, in <module>\n",
    "    automatisation(datasets, algorithms)\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 432, in automatisation\n",
    "    df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 308, in benchmark\n",
    "    res = algo_runner(bunch, params)\n",
    "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/4eme/stage/tests/benchmark.py\", line 184, in run_vae\n",
    "    VAE_model.fit(x)\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/base_dl.py\", line 202, in fit\n",
    "    self.decision_scores_ = self.decision_function(X)\n",
    "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/base_dl.py\", line 290, in decision_function\n",
    "    anomaly_scores = self.evaluate(data_loader)\n",
    "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/base_dl.py\", line 315, in evaluate\n",
    "    score = self.evaluating_forward(batch_data)\n",
    "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/models/vae.py\", line 264, in evaluating_forward\n",
    "    score = pairwise_distances_no_broadcast(x.numpy(),\n",
    "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/pyod/utils/stat_models.py\", line 43, in pairwise_distances_no_broadcast\n",
    "    Y = check_array(Y)\n",
    "        ^^^^^^^^^^^^^^\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1107, in check_array\n",
    "    _assert_all_finite(\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n",
    "    _assert_all_finite_element_wise(\n",
    "  File \"/home/khalil/ls/envs/sfml/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n",
    "    raise ValueError(msg_err)\n",
    "ValueError: Input contains NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb63ac0-4a21-428d-8a28-52551c477f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech has NaN?  False\n",
      "satellite has NaN?  False\n",
      "fraude has NaN?  False\n",
      "cancer du sein has NaN?  False\n",
      "shuttle has NaN?  False\n"
     ]
    }
   ],
   "source": [
    "for bunch in datasets:\n",
    "    has_nan = np.isnan(bunch.data).any()\n",
    "    print(f\"{bunch.name} has NaN?  {has_nan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f431c-a794-48bc-95f0-45f0e9d59826",
   "metadata": {},
   "source": [
    "### Benchmark with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e9ceb0-c737-48ab-9a99-afe6e497d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint_csv(algorithms, auc_csv, recall_csv,time_csv):\n",
    "    os.makedirs(\"./checkpoints\", exist_ok=True) # Create the repertories \n",
    "    algo_names  = [name for name, _, _ in algorithms]\n",
    "    cols = [\"dataset\"] + algo_names\n",
    "    \n",
    "    # Create empty CSVs with header row\n",
    "    pd.DataFrame(columns=cols).to_csv(auc_csv,  mode=\"w\",  index=False)\n",
    "    pd.DataFrame(columns=cols).to_csv(recall_csv, mode=\"w\", index=False)\n",
    "    pd.DataFrame(columns=cols).to_csv(time_csv, mode=\"w\",  index=False)\n",
    "\n",
    "def append_dict_to_csv(row, path): \n",
    "    \"\"\"\n",
    "        Add a row in a csv file, I use pandas because it's easy to write and I have a small number of dataset\n",
    "    \"\"\"\n",
    "    pd.DataFrame([row]).to_csv(path, mode=\"a\", header=False, index=False, float_format=\"%.2f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a122c023-f8d9-4815-94d3-9f0c05a28e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_checkpoint_csv(algorithms,\"./checkpoints/auc_checkpoint.csv\", \"./checkpoints/recall_checkpoint.csv\",\"./checkpoints/time_checkpoint.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc63889-5021-4bdb-8a99-2db1d5f700ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech  : départ\n",
      "   IForest : départ\n",
      "AUC : 0.47671226681741097 RAPPEL : 0.01639344262295082 TEMPS : 0.1239963070001977\n",
      "   IForest : fini\n",
      "   LOF : départ\n",
      "AUC : 0.5078168456755229 RAPPEL : 0.0 TEMPS : 0.1987326070011477\n",
      "   LOF : fini\n",
      "speech  : fini\n",
      "cancerS  : départ\n",
      "   IForest : départ\n",
      "AUC : 0.973109243697479 RAPPEL : 0.8 TEMPS : 0.09633737500189454\n",
      "   IForest : fini\n",
      "   LOF : départ\n",
      "AUC : 0.9845938375350141 RAPPEL : 0.9 TEMPS : 0.0030664250007248484\n",
      "   LOF : fini\n",
      "cancerS  : fini\n",
      "donneurs  : départ\n",
      "   IForest : départ\n",
      "AUC : 0.7805672089358378 RAPPEL : 0.7719422500681014 TEMPS : 2.8159428920007485\n",
      "   IForest : fini\n",
      "   LOF : départ\n"
     ]
    }
   ],
   "source": [
    "def benchmark_checkpoints(datasets, algorithms, auc_csv=\"./checkpoints/auc_checkpoint.csv\", recall_csv=\"./checkpoints/recall_checkpoint.csv\",\n",
    "              time_csv=\"./checkpoints/time_checkpoint.csv\"):\n",
    "    \"\"\"\n",
    "    Run a benchmark of multiple algorithms on multiple datasets.\n",
    "    \n",
    "     datasets : list of Bunch\n",
    "            Each Bunch must have attributes:\n",
    "              - name : dataset name\n",
    "              - data : feature data\n",
    "              - target : ground truth labels\n",
    "        algorithms : list of tuples\n",
    "            Each tuple is (algo_name, runner_function, params_dict):\n",
    "              - algo_name : descriptive name of the algorithm\n",
    "              - runner_function : function(bunch, params) - > dict\n",
    "              - params_dict: parameters to pass to the runner\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_auc : pandas.DataFrame\n",
    "        AUC scores for each (dataset × algorithm).\n",
    "    df_recall : pandas.DataFrame\n",
    "        Recall scores for each (dataset × algorithm).\n",
    "    df_time : pandas.DataFrame\n",
    "        Execution time (seconds) for each (dataset × algorithm).\n",
    "    \"\"\"\n",
    "\n",
    "    create_checkpoint_csv(algorithms, auc_csv, recall_csv,time_csv) ### <- <-\n",
    "    \n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for bunch in datasets:\n",
    "        print(bunch.name, \" : départ\")\n",
    "        auc_info, recall_info, time_info = {\"dataset\": bunch.name}, {\"dataset\": bunch.name}, {\"dataset\": bunch.name}\n",
    "        \n",
    "        for algo_name, algo_runner, params in algorithms:\n",
    "            print(\"  \", algo_name, \": départ\")\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[algo_name]    = res['auc']\n",
    "            recall_info[algo_name] = res['recall']\n",
    "            time_info[algo_name]   = res['time']\n",
    "            \n",
    "            print(\"AUC :\", res['auc'], \"RAPPEL :\", res['recall'],\"TEMPS :\", res['time'])\n",
    "            print(\"  \", algo_name, \": fini\")\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "\n",
    "        append_dict_to_csv(auc_info,auc_csv) ### <- <-\n",
    "        append_dict_to_csv(recall_info,recall_csv)\n",
    "        append_dict_to_csv(time_info,time_csv)\n",
    "           \n",
    "        print(bunch.name, \" : fini\")\n",
    "\n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "    \n",
    "    return df_auc, df_recall, df_time\n",
    "\n",
    "\n",
    "benchmark_checkpoints(datasets, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4a397-64ef-4f01-ac05-043a98c1f24f",
   "metadata": {},
   "source": [
    "### Function Benchmark 2, where we do for each algo all the datasets first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d775f009-324e-42b1-b22b-5107b1577de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint_csv2(datasets, auc_csv, recall_csv,time_csv):\n",
    "    os.makedirs(\"./checkpoints\", exist_ok=True) # Create the repertories \n",
    "    dataset_names = [b.name for b in datasets]\n",
    "    cols = [\"algorithm\"] + dataset_names\n",
    "\n",
    "    # write empty CSVs with that header (mode=\"w\" to overwrite)\n",
    "    pd.DataFrame(columns=cols).to_csv(   auc_csv,    mode=\"w\", index=False)\n",
    "    pd.DataFrame(columns=cols).to_csv(   recall_csv, mode=\"w\", index=False)\n",
    "    pd.DataFrame(columns=cols).to_csv(   time_csv,   mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351ea813-3490-44ce-b592-aa39143dd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_algo_dataset(datasets, algorithms, auc_csv=\"./checkpoints/auc_checkpoint.csv\", recall_csv=\"./checkpoints/recall_checkpoint.csv\",\n",
    "              time_csv=\"./checkpoints/time_checkpoint.csv\"):\n",
    "    \"\"\"\n",
    "    Run a benchmark of multiple algorithms on multiple datasets.\n",
    "    \n",
    "     datasets : list of Bunch\n",
    "            Each Bunch must have attributes:\n",
    "              - name : dataset name\n",
    "              - data : feature data\n",
    "              - target : ground truth labels\n",
    "        algorithms : list of tuples\n",
    "            Each tuple is (algo_name, runner_function, params_dict):\n",
    "              - algo_name : descriptive name of the algorithm\n",
    "              - runner_function : function(bunch, params) - > dict\n",
    "              - params_dict: parameters to pass to the runner\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_auc : pandas.DataFrame\n",
    "        AUC scores for each (dataset × algorithm).\n",
    "    df_recall : pandas.DataFrame\n",
    "        Recall scores for each (dataset × algorithm).\n",
    "    df_time : pandas.DataFrame\n",
    "        Execution time (seconds) for each (dataset × algorithm).\n",
    "    \"\"\"\n",
    "\n",
    "    create_checkpoint_csv2(datasets, auc_csv, recall_csv, time_csv)\n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for algo_name, algo_runner, params in algorithms:\n",
    "        print(algo_name, \" : départ\")\n",
    "        auc_info, recall_info, time_info    = {\"algorithm\": algo_name}, {\"algorithm\": algo_name},{\"algorithm\": algo_name}        \n",
    "        \n",
    "        for bunch in datasets:\n",
    "        \n",
    "            print(\"  \", bunch.name, \": départ\")\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[bunch.name]    = res[\"auc\"]\n",
    "            recall_info[bunch.name] = res[\"recall\"]\n",
    "            time_info[bunch.name]   = res[\"time\"]\n",
    "\n",
    "            print(\"  \", \"AUC :\", res['auc'], \"RAPPEL :\", res['recall'],\"TEMPS :\", res['time'])\n",
    "            print(\"  \", bunch.name, \": fini\")\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "        print(algo_name, \" : fini\")\n",
    "\n",
    "        append_dict_to_csv(auc_info,auc_csv) \n",
    "        append_dict_to_csv(recall_info,recall_csv)\n",
    "        append_dict_to_csv(time_info,time_csv)\n",
    "        \n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "    \n",
    "    return df_auc, df_recall, df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4335eec3-9c0b-4620-8afe-65d5e5909454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IForest  : départ\n",
      "   speech : départ\n",
      "   AUC : 0.46924590163934415 RAPPEL : 0.0 TEMPS : 0.12134188200070639\n",
      "   speech : fini\n",
      "   cancerS : départ\n",
      "   AUC : 0.977310924369748 RAPPEL : 0.8 TEMPS : 0.08650581199981389\n",
      "   cancerS : fini\n",
      "   donneurs : départ\n",
      "   AUC : 0.7769625451673147 RAPPEL : 0.8348951239444293 TEMPS : 2.9444997900027374\n",
      "   donneurs : fini\n",
      "   satellite : départ\n",
      "   AUC : 0.9478872305140963 RAPPEL : 0.8266666666666667 TEMPS : 0.13317388399809715\n",
      "   satellite : fini\n",
      "   fraude : départ\n",
      "   AUC : 0.9495011258696376 RAPPEL : 0.8313008130081301 TEMPS : 1.2327636670015636\n",
      "   fraude : fini\n",
      "   http : départ\n",
      "   AUC : 0.9717553867904851 RAPPEL : 0.9429657794676806 TEMPS : 1.9389586059987778\n",
      "   http : fini\n",
      "   shuttle : départ\n",
      "   AUC : 0.997285962890537 RAPPEL : 0.9817767653758542 TEMPS : 0.2845184979996702\n",
      "   shuttle : fini\n",
      "IForest  : fini\n"
     ]
    }
   ],
   "source": [
    "df1, df2, df3 =benchmark_algo_dataset(datasets, [algorithms[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951965c6-28ca-4f46-a718-fd6645d9fbe7",
   "metadata": {},
   "source": [
    "### Rank directly in latex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0aaf003-6024-4664-8143-e29256a93978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hlines(latex):\n",
    "    return latex.replace(\"\\\\\\\\\", \"\\\\\\\\ \\\\hline\")\n",
    "        \n",
    "def pandas_to_latex(df_auc, df_recall, df_time):\n",
    "    latex_auc = add_hlines(df_auc.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_recall = add_hlines(df_recall.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_time = add_hlines(df_time.to_latex(index=False,float_format=\"%.2f\"))\n",
    "\n",
    "    return latex_auc, latex_recall, latex_time\n",
    "\n",
    "def pandas_to_latex_with_ranks(df):\n",
    "    print(df)\n",
    "    # colonne seulement avec des nombres pour la comparaison \n",
    "    num_cols = df.select_dtypes(include=\"number\").columns\n",
    "    print(num_cols)\n",
    "    # Comparaison \n",
    "    ranks = df[num_cols].rank(axis=1, method='dense', ascending=False).astype(int)\n",
    "    print(ranks)\n",
    "\n",
    "    # \n",
    "    vals = df[num_cols].round(2)\n",
    "    print(vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16400167-6e87-4ccc-97ef-4ed59e4680cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  algorithm    speech   cancerS  donneurs  satellite    fraude      http  \\\n",
      "0   IForest  0.469246  0.977311  0.776963   0.947887  0.949501  0.971755   \n",
      "\n",
      "    shuttle  \n",
      "0  0.997286  \n",
      "Index(['speech', 'cancerS', 'donneurs', 'satellite', 'fraude', 'http',\n",
      "       'shuttle'],\n",
      "      dtype='object')\n",
      "   speech  cancerS  donneurs  satellite  fraude  http  shuttle\n",
      "0       7        2         6          5       4     3        1\n",
      "   speech  cancerS  donneurs  satellite  fraude  http  shuttle\n",
      "0    0.47     0.98      0.78       0.95    0.95  0.97      1.0\n"
     ]
    }
   ],
   "source": [
    "pandas_to_latex_with_ranks(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc022d0d-4b39-4a92-b75a-1cad36557c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
