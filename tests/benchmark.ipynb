{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8fc678f-57a4-4c83-84aa-e5fff4ce5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import Bunch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8d043-a73a-4740-9576-e199adfed51b",
   "metadata": {},
   "source": [
    "### Datas preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "670c3313-d670-4bf5-bd5f-7c2b9d5f547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data_rapport/speech.csv', '../data_rapport/kddcup99 http.csv', '../data_rapport/kddcup2014 donneurs .csv', '../data_rapport/satellite.csv', '../data_rapport/fraude.csv', '../data_rapport/cancer du sein.csv', '../data_rapport/shuttle.csv']\n"
     ]
    }
   ],
   "source": [
    "def csv_data_into_bunch(filenames):\n",
    "    \"\"\"\n",
    "    This function assume that the csv file will have label egal to 1 for anomalie and 0 for normal value.\n",
    "    The csv have a header too.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for i in filenames:\n",
    "        name = os.path.splitext(os.path.basename(i))[0]\n",
    "\n",
    "        data = pd.read_csv(i).to_numpy()\n",
    "    \n",
    "        x = data[:,:-1]\n",
    "        y = data[:,-1]\n",
    "\n",
    "        # Standardisation\n",
    "        scaler = StandardScaler()\n",
    "        x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "        # Store in a Bunch\n",
    "        datasets.append(Bunch(name=name, data=x_scaled, target=y))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "        \n",
    "def list_all_file_name_in_folder(folder=\"../data_rapport\"):\n",
    "    return [os.path.join(folder, filename) for filename in os.listdir(folder) if filename.endswith(\".csv\")]\n",
    "\n",
    "filenames = list_all_file_name_in_folder()\n",
    "print(filenames)\n",
    "\n",
    "datasets = csv_data_into_bunch(filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bea4f0-fa66-4086-a99c-ae61f867213a",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "This function takes two parameters: `datasets` and `algorithms`.\n",
    "\n",
    "- `datasets`: a list of `Bunch` objects, each containing:\n",
    "  - `name`: the name of the dataset  \n",
    "  - `data`: the feature matrix (X)  \n",
    "  - `target`: the label array (y)\n",
    "\n",
    "\n",
    "\n",
    "- `algorithms`: a list of tuples in the following form:\n",
    "\n",
    "  ```python\n",
    "  algorithms = [\n",
    "      (\"name\", # Algorithm name \n",
    "       run_name, # runner function\n",
    "       {\"param1\": ..., \"param2\": ...}), # dict of paramaters \n",
    "      # …\n",
    "  ]\n",
    "\n",
    "This function will return three pandas dataFramme : auc, recall, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5a6a6d4-1a09-4525-bfa2-0763759b18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(datasets, algorithms):\n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for bunch in datasets:\n",
    "        print(bunch.name, \" : départ\")\n",
    "        auc_info, recall_info, time_info = {\"dataset\": bunch.name}, {\"dataset\": bunch.name}, {\"dataset\": bunch.name}\n",
    "        \n",
    "        for algo_name, algo_runner, params in algorithms:\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[algo_name]    = res['auc']\n",
    "            recall_info[algo_name] = res['recall']\n",
    "            time_info[algo_name]   = res['time']\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "        print(bunch.name, \" : fini\")\n",
    "        \n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "    \n",
    "    return df_auc, df_recall, df_time\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986e4b9-bc87-4c78-8869-8f61fbfd9b35",
   "metadata": {},
   "source": [
    "### Algorithms \n",
    "\n",
    "Each runner function must accept exactly two arguments and return a dictionary with the same keys:\n",
    "\n",
    "\n",
    "- `Parameters`\n",
    "\n",
    "    - `bunch` : like in the datasets list of the benchmark parameters\n",
    "    \n",
    "    - `params` :  like in the algorithms list of the benchmark parameters\n",
    "\n",
    "- `Returns`\n",
    "\n",
    "- `dict`  \n",
    "  ```python\n",
    "  {\n",
    "    \"auc\":    <float>,  // ROC AUC score\n",
    "    \"recall\": <float>,  // Recall score\n",
    "    \"time\":   <float>   // Elapsed time (seconds)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e0a6219-ac47-4f29-9b5f-4979470dcf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iforest(bunch, params):\n",
    "\n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    IF = IsolationForest(**params)\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    IF.fit(x)\n",
    "    y_pred = IF.predict(x) \n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -IF.decision_function(x)\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }\n",
    "\n",
    "\n",
    "def run_lof(bunch, params) :\n",
    "    \"\"\"\n",
    "    bunch : Sklearn object witch contains the 'name', the 'data' and the 'target'(label) of the dataset\n",
    "    params : dictionary of the parameters needed for this algorithm\n",
    "    \"\"\"\n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    LOF = LocalOutlierFactor(** params)\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    y_pred = LOF.fit_predict(x)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -LOF.negative_outlier_factor_\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bdeba1-32ad-4eee-bc57-319e704972d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech  : départ\n",
      "speech  : fini\n",
      "kddcup99 http  : départ\n",
      "kddcup99 http  : fini\n",
      "kddcup2014 donneurs   : départ\n"
     ]
    }
   ],
   "source": [
    "algorithms = [\n",
    "     (\"IForest\", run_iforest, {'n_estimators':200, 'contamination':0.1}),\n",
    "     (\"LOF\", run_lof, {'n_neighbors':250, 'contamination':0.1}),\n",
    "]\n",
    "\n",
    "df_auc, df_recall, df_time = benchmark(datasets, algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528a2a9-a67f-4948-8cbd-55e9733c2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hlines(latex):\n",
    "    return latex.replace(\"\\\\\\\\\", \"\\\\\\\\ \\\\hline\")\n",
    "        \n",
    "def pandas_to_latex(df_auc, df_recall, df_time):\n",
    "\n",
    "    latex_auc = add_hlines(df_auc.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_recall = add_hlines(df_recall.to_latex(index=False,float_format=\"%.2f\"))\n",
    "    latex_time = add_hlines(df_time.to_latex(index=False,float_format=\"%.2f\"))\n",
    "\n",
    "    return latex_auc, latex_recall, latex_time\n",
    "\n",
    "latex_auc, latex_recall, latex_time = pandas_to_latex(df_auc, df_recall, df_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe7053-7fee-4b76-bc83-b264fa370534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_latex_in_file(latex_auc, latex_recall,latex_time, algorithms, filepath = \"./latex_results.txt\"):\n",
    "    # Actual time\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Informations about algorithms used : name1 : param1 = 0, param2=0 ; ...\n",
    "    infos = []\n",
    "    for name, _, params in algorithms:\n",
    "        param_str = \", \".join(f\"{k}={v}\" for k, v in params.items())\n",
    "        infos.append(f\"{name}: {param_str}\")\n",
    "    info_algo = \"; \".join(infos)\n",
    "\n",
    "    content = (\n",
    "        f\"{now}\\n\"\n",
    "        f\"{info_algo}\\n\\n\"\n",
    "        r\"%=== AUC table ===\" \"\\n\"\n",
    "        f\"{latex_auc}\\n\\n\"\n",
    "        r\"%=== Recall table ===\" \"\\n\"\n",
    "        f\"{latex_recall}\\n\\n\"\n",
    "        r\"%=== Time table ===\" \"\\n\"\n",
    "        f\"{latex_time}\\n\"\n",
    "    )\n",
    "\n",
    "    # Add at the start of the file without delete the old content \n",
    "    old = \"\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            old = f.read()\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(content)\n",
    "            f.write(old)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "446493a2-fc6b-445e-8c7d-2ead6752ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "0 & 1 & 2 \\\\\n",
      "\\midrule\n",
      "test1 & 1 & 2 \\\\\n",
      "test2 & 2 & 3 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "0 & 1 & 2 \\\\ \\hline\n",
      "\\midrule\n",
      "test1 & 1 & 2 \\\\ \\hline\n",
      "test2 & 2 & 3 \\\\ \\hline\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tab = [['test1', 1, 2], ['test2', 2, 3]]\n",
    "df = pd.DataFrame(tab)\n",
    "print(df.to_latex(index=False,float_format=\"%.2f\"))\n",
    "print(add_hlines(df.to_latex(index=False,float_format=\"%.2f\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae97943-c58e-49e8-81b4-57b13281e5a9",
   "metadata": {},
   "source": [
    "### Combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bece84-38c4-4ae3-b28f-26f3ed4b142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatisation(datasets, algorithms, filepath = \"./latex_results.txt\"):\n",
    "    # Calculate all the metrics\n",
    "    df_auc, df_recall, df_time = benchmark(datasets, algorithms)\n",
    "\n",
    "    # Transform the metrics into latex tab \n",
    "    latex_auc, latex_recall, latex_time = pandas_to_latex(df_auc, df_recall, df_time)\n",
    "\n",
    "    # Write these latex tab in a file\n",
    "    write_latex_in_file(latex_auc, latex_recall,latex_time, algorithms, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b526246-4be2-4912-9383-24ec56cd55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "automatisation(datasets, algorithms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
