{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fc678f-57a4-4c83-84aa-e5fff4ce5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import Bunch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8d043-a73a-4740-9576-e199adfed51b",
   "metadata": {},
   "source": [
    "### Datas preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670c3313-d670-4bf5-bd5f-7c2b9d5f547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data_rapport/satellite-unsupervised-ad.csv', '../data_rapport/shuttle-unsupervised-ad.csv', '../data_rapport/kdd99-unsupervised-ad.csv', '../data_rapport/KDD2014_donors_10feat_nomissing_normalised.csv', '../data_rapport/breast-cancer-unsupervised-ad.csv', '../data_rapport/speech-unsupervised-ad.csv', '../data_rapport/creditcardfraud_normalised.csv']\n"
     ]
    }
   ],
   "source": [
    "def csv_data_into_bunch(filenames):\n",
    "    \"\"\"\n",
    "    This function assume that the csv file will have label egal to 1 for anomalie and 0 for normal value.\n",
    "    The csv have a header too.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for i in filenames:\n",
    "        name = os.path.splitext(os.path.basename(i))[0]\n",
    "\n",
    "        data = pd.read_csv(i).to_numpy()\n",
    "    \n",
    "        x = data[:,:-1]\n",
    "        y = data[:,-1]\n",
    "\n",
    "        # Standardisation\n",
    "        scaler = StandardScaler()\n",
    "        x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "        # Store in a Bunch\n",
    "        datasets.append(Bunch(name=name, data=x_scaled, target=y))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "        \n",
    "def list_all_file_name_in_folder(folder=\"../data_rapport\"):\n",
    "    return [os.path.join(folder, filename) for filename in os.listdir(folder) if filename.endswith(\".csv\")]\n",
    "\n",
    "filenames = list_all_file_name_in_folder()\n",
    "print(filenames)\n",
    "\n",
    "datasets = csv_data_into_bunch(filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bea4f0-fa66-4086-a99c-ae61f867213a",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "This function takes two parameters: `datasets` and `algorithms`.\n",
    "\n",
    "- `datasets`: a list of `Bunch` objects, each containing:\n",
    "  - `name`: the name of the dataset  \n",
    "  - `data`: the feature matrix (X)  \n",
    "  - `target`: the label array (y)\n",
    "\n",
    "\n",
    "\n",
    "- `algorithms`: a list of tuples in the following form:\n",
    "\n",
    "  ```python\n",
    "  algorithms = [\n",
    "      (\"name\", # Algorithm name \n",
    "       run_name, # runner function\n",
    "       {\"param1\": ..., \"param2\": ...}), # dict of paramaters \n",
    "      # â€¦\n",
    "  ]\n",
    "\n",
    "This function will return three pandas dataFramme : auc, recall, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a6a6d4-1a09-4525-bfa2-0763759b18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(datasets, algorithms):\n",
    "    auc_list, recall_list, time_list = [], [], []\n",
    "    \n",
    "    for bunch in datasets:\n",
    "        auc_info, recall_info, time_info = {\"dataset\": bunch.name}, {\"dataset\": bunch.name}, {\"dataset\": bunch.name}\n",
    "        \n",
    "        for algo_name, algo_runner, params in algorithms:\n",
    "            res = algo_runner(bunch, params) \n",
    "            \n",
    "            auc_info[algo_name]    = res['auc']\n",
    "            recall_info[algo_name] = res['recall']\n",
    "            time_info[algo_name]   = res['time']\n",
    "            \n",
    "        auc_list.append(auc_info)\n",
    "        recall_list.append(recall_info)\n",
    "        time_list.append(time_info)\n",
    "\n",
    "    print(bunch.name, \" : fini\")\n",
    "    df_auc    = pd.DataFrame(auc_list)\n",
    "    df_recall = pd.DataFrame(recall_list)\n",
    "    df_time   = pd.DataFrame(time_list)\n",
    "\n",
    "    print(df_auc)\n",
    "    \n",
    "    return df_auc, df_recall, df_time\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986e4b9-bc87-4c78-8869-8f61fbfd9b35",
   "metadata": {},
   "source": [
    "### Algorithms \n",
    "\n",
    "Each runner function must accept exactly two arguments and return a dictionary with the same keys:\n",
    "\n",
    "\n",
    "- `Parameters`\n",
    "\n",
    "    - `bunch` : like in the datasets list of the benchmark parameters\n",
    "    \n",
    "    - `params` :  like in the algorithms list of the benchmark parameters\n",
    "\n",
    "- `Returns`\n",
    "\n",
    "- `dict`  \n",
    "  ```python\n",
    "  {\n",
    "    \"auc\":    <float>,  // ROC AUC score\n",
    "    \"recall\": <float>,  // Recall score\n",
    "    \"time\":   <float>   // Elapsed time (seconds)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0a6219-ac47-4f29-9b5f-4979470dcf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iforest(bunch, params):\n",
    "\n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    IF = IsolationForest(**params)\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    IF.fit(x)\n",
    "    y_pred = IF.predict(x) \n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -IF.decision_function(x)\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }\n",
    "\n",
    "\n",
    "def run_lof(bunch, params) :\n",
    "    \"\"\"\n",
    "    bunch : Sklearn object witch contains the 'name', the 'data' and the 'target'(label) of the dataset\n",
    "    params : dictionary of the parameters needed for this algorithm\n",
    "    \"\"\"\n",
    "    x, y_ground_truth = bunch.data, bunch.target\n",
    "    LOF = LocalOutlierFactor(** params)\n",
    "    \n",
    "    # Fit + predict \n",
    "    start = time.perf_counter()\n",
    "    y_pred = LOF.fit_predict(x)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Binairisation of the prediction and scores calculation \n",
    "    y_pred = (y_pred == -1)\n",
    "    scores = -LOF.negative_outlier_factor_\n",
    "\n",
    "    # metrics\n",
    "    auc = roc_auc_score(y_ground_truth, scores)\n",
    "    recall = recall_score(y_ground_truth, y_pred)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'recall': recall,\n",
    "        'time' : elapsed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bdeba1-32ad-4eee-bc57-319e704972d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "     (\"IForest\", run_iforest, {'n_estimators':100, 'contamination':0.1}),\n",
    "     (\"LOF\", run_lof, {'n_neighbors':150, 'contamination':0.1}),\n",
    "]\n",
    "\n",
    "df_auc, df_recall, df_time = benchmark(datasets, algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b5364-b3aa-4d77-9d39-0d9f49ce25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_to_latex(df_auc, df_recall, df_time):\n",
    "    latex_auc = df_auc.to_latex(index=False,header=False,float_format=\"%.3f\",line_terminator='\\\\\\\\ \\\\hline\\n')\n",
    "    latex_recall = df_recall.to_latex(index=False,header=False,float_format=\"%.3f\",line_terminator='\\\\\\\\ \\\\hline\\n')\n",
    "    latex_time = df_time.to_latex(index=False,header=False,float_format=\"%.3f\",line_terminator='\\\\\\\\ \\\\hline\\n')\n",
    "\n",
    "    print(latex_auc, latex_recall, latex_time)\n",
    "\n",
    "pandas_to_latex(df_auc, df_recall, df_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446493a2-fc6b-445e-8c7d-2ead6752ab45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
